{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1F9XxSN05aDEQQ4Rm2cMI80wUJWDqhTfP",
      "authorship_tag": "ABX9TyNbDnSigbqe1Bp1lgOt6yYh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MDankloff/Cluster-Bias-Disco/blob/main/BAF_Reproduce.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "_OU0z3wh7tWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-colab"
      ],
      "metadata": {
        "id": "J6de3R-x-1nX",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54e16671-7a80-431a-b0ee-79eaeedd79f0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-colab in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: google-auth==2.27.0 in /usr/local/lib/python3.10/dist-packages (from google-colab) (2.27.0)\n",
            "Requirement already satisfied: ipykernel==5.5.6 in /usr/local/lib/python3.10/dist-packages (from google-colab) (5.5.6)\n",
            "Requirement already satisfied: ipyparallel==8.8.0 in /usr/local/lib/python3.10/dist-packages (from google-colab) (8.8.0)\n",
            "Requirement already satisfied: ipython==7.34.0 in /usr/local/lib/python3.10/dist-packages (from google-colab) (7.34.0)\n",
            "Requirement already satisfied: notebook==6.5.5 in /usr/local/lib/python3.10/dist-packages (from google-colab) (6.5.5)\n",
            "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.10/dist-packages (from google-colab) (2.2.2)\n",
            "Requirement already satisfied: portpicker==1.5.2 in /usr/local/lib/python3.10/dist-packages (from google-colab) (1.5.2)\n",
            "Requirement already satisfied: requests==2.32.3 in /usr/local/lib/python3.10/dist-packages (from google-colab) (2.32.3)\n",
            "Requirement already satisfied: tornado==6.3.3 in /usr/local/lib/python3.10/dist-packages (from google-colab) (6.3.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.27.0->google-colab) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.27.0->google-colab) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.27.0->google-colab) (4.9)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel==5.5.6->google-colab) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel==5.5.6->google-colab) (5.7.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel==5.5.6->google-colab) (6.1.12)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipyparallel==8.8.0->google-colab) (4.4.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from ipyparallel==8.8.0->google-colab) (0.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ipyparallel==8.8.0->google-colab) (5.9.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from ipyparallel==8.8.0->google-colab) (2.8.2)\n",
            "Requirement already satisfied: pyzmq>=18 in /usr/local/lib/python3.10/dist-packages (from ipyparallel==8.8.0->google-colab) (24.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from ipyparallel==8.8.0->google-colab) (4.66.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (71.0.4)\n",
            "Collecting jedi>=0.16 (from ipython==7.34.0->google-colab)\n",
            "  Using cached jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython==7.34.0->google-colab) (4.9.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (3.1.4)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (5.7.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (0.21.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook==6.5.5->google-colab) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.2->google-colab) (1.26.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.2->google-colab) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.2->google-colab) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.32.3->google-colab) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.32.3->google-colab) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.32.3->google-colab) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.32.3->google-colab) (2024.8.30)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython==7.34.0->google-colab) (0.8.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook==6.5.5->google-colab) (4.3.6)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook==6.5.5->google-colab) (0.2.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (0.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (2.1.5)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (0.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (24.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook==6.5.5->google-colab) (1.3.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook==6.5.5->google-colab) (2.20.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook==6.5.5->google-colab) (4.23.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython==7.34.0->google-colab) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython==7.34.0->google-colab) (0.2.13)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth==2.27.0->google-colab) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.1->ipyparallel==8.8.0->google-colab) (1.16.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook==6.5.5->google-colab) (21.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.5->google-colab) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.5->google-colab) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.5->google-colab) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.5->google-colab) (0.20.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook==6.5.5->google-colab) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook==6.5.5->google-colab) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook==6.5.5->google-colab) (2.6)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook==6.5.5->google-colab) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook==6.5.5->google-colab) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook==6.5.5->google-colab) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook==6.5.5->google-colab) (1.8.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook==6.5.5->google-colab) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook==6.5.5->google-colab) (1.2.2)\n",
            "Using cached jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "Installing collected packages: jedi\n",
            "Successfully installed jedi-0.19.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install aequitas"
      ],
      "metadata": {
        "id": "kDIVzjkH-5xt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bf1edd63-31d3-4781-9454-7d340cf5ed3f",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting aequitas\n",
            "  Downloading aequitas-1.0.0-3-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting matplotlib>=3.7.5 (from aequitas)\n",
            "  Downloading matplotlib-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pandas>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from aequitas) (2.2.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.2 in /usr/local/lib/python3.10/dist-packages (from aequitas) (6.0.2)\n",
            "Collecting seaborn>=0.13.2 (from aequitas)\n",
            "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting altair>=5.4.0 (from aequitas)\n",
            "  Downloading altair-5.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting millify>=0.1.1 (from aequitas)\n",
            "  Downloading millify-0.1.1.tar.gz (1.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from aequitas) (1.13.1)\n",
            "Collecting optuna>=3.6.1 (from aequitas)\n",
            "  Downloading optuna-4.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aif360>=0.6.1 (from aequitas)\n",
            "  Downloading aif360-0.6.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: lightgbm>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from aequitas) (4.5.0)\n",
            "Collecting fairgbm>=0.9.14 (from aequitas)\n",
            "  Downloading fairgbm-0.9.14-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting fairlearn>=0.10.0 (from aequitas)\n",
            "  Downloading fairlearn-0.10.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting hydra-core>=1.3.2 (from aequitas)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting hyperparameter-tuning>=0.3.1 (from aequitas)\n",
            "  Downloading hyperparameter_tuning-0.3.2-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting validators>=0.33.0 (from aequitas)\n",
            "  Downloading validators-0.34.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from aequitas) (1.26.4)\n",
            "Collecting fastparquet>=2024.2.0 (from aequitas)\n",
            "  Downloading fastparquet-2024.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: requests>=2.32.3 in /usr/local/lib/python3.10/dist-packages (from aequitas) (2.32.3)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from aequitas) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.10/dist-packages (from aif360>=0.6.1->aequitas) (1.5.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair>=5.4.0->aequitas) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair>=5.4.0->aequitas) (4.23.0)\n",
            "Collecting narwhals>=1.5.2 (from altair>=5.4.0->aequitas)\n",
            "  Downloading narwhals-1.9.2-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from altair>=5.4.0->aequitas) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from altair>=5.4.0->aequitas) (4.12.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from fairgbm>=0.9.14->aequitas) (0.44.0)\n",
            "Collecting cramjam>=2.3 (from fastparquet>=2024.2.0->aequitas)\n",
            "  Downloading cramjam-2.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from fastparquet>=2024.2.0->aequitas) (2024.6.1)\n",
            "Collecting omegaconf<2.4,>=2.2 (from hydra-core>=1.3.2->aequitas)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.3.2->aequitas)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting schema (from hyperparameter-tuning>=0.3.1->aequitas)\n",
            "  Downloading schema-0.7.7-py2.py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.5->aequitas) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.5->aequitas) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.5->aequitas) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.5->aequitas) (1.4.7)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.5->aequitas) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.5->aequitas) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.5->aequitas) (2.8.2)\n",
            "Collecting alembic>=1.5.0 (from optuna>=3.6.1->aequitas)\n",
            "  Downloading alembic-1.13.3-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna>=3.6.1->aequitas)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=3.6.1->aequitas) (2.0.35)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna>=3.6.1->aequitas) (4.66.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.3->aequitas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.3->aequitas) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.3->aequitas) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.3->aequitas) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.3->aequitas) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.3->aequitas) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->aequitas) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->aequitas) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->aequitas) (3.3)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna>=3.6.1->aequitas)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=5.4.0->aequitas) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=5.4.0->aequitas) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=5.4.0->aequitas) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=5.4.0->aequitas) (0.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.5->aequitas) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->aif360>=0.6.1->aequitas) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->aif360>=0.6.1->aequitas) (3.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna>=3.6.1->aequitas) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair>=5.4.0->aequitas) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.4.0->aequitas) (1.3.0)\n",
            "Downloading aequitas-1.0.0-3-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aif360-0.6.1-py3-none-any.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.7/259.7 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading altair-5.4.1-py3-none-any.whl (658 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m658.1/658.1 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fairgbm-0.9.14-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fairlearn-0.10.0-py3-none-any.whl (234 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m234.1/234.1 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastparquet-2024.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hyperparameter_tuning-0.3.2-py3-none-any.whl (26 kB)\n",
            "Downloading matplotlib-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optuna-4.0.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.8/362.8 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading validators-0.34.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.3-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.2/233.2 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cramjam-2.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading narwhals-1.9.2-py3-none-any.whl (185 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.1/185.1 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Downloading schema-0.7.7-py2.py3-none-any.whl (18 kB)\n",
            "Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime, millify\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=3b93e4cd44cf203cb071b397c77f6dc695e2dda886e73ac0494390b844a7cc2d\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "  Building wheel for millify (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for millify: filename=millify-0.1.1-py3-none-any.whl size=1844 sha256=9e6ec61fc18a92c569db5d27cab6a76794b0d465f65570a00276d66589b55ffe\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/8f/53/2759feac2e247ce89c1165c3ff12d484de7714a875ea3464f0\n",
            "Successfully built antlr4-python3-runtime millify\n",
            "Installing collected packages: schema, millify, antlr4-python3-runtime, validators, omegaconf, narwhals, Mako, cramjam, colorlog, matplotlib, hydra-core, alembic, seaborn, optuna, fastparquet, fairlearn, fairgbm, aif360, hyperparameter-tuning, altair, aequitas\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.7.1\n",
            "    Uninstalling matplotlib-3.7.1:\n",
            "      Successfully uninstalled matplotlib-3.7.1\n",
            "  Attempting uninstall: seaborn\n",
            "    Found existing installation: seaborn 0.13.1\n",
            "    Uninstalling seaborn-0.13.1:\n",
            "      Successfully uninstalled seaborn-0.13.1\n",
            "  Attempting uninstall: altair\n",
            "    Found existing installation: altair 4.2.2\n",
            "    Uninstalling altair-4.2.2:\n",
            "      Successfully uninstalled altair-4.2.2\n",
            "Successfully installed Mako-1.3.5 aequitas-1.0.0 aif360-0.6.1 alembic-1.13.3 altair-5.4.1 antlr4-python3-runtime-4.9.3 colorlog-6.8.2 cramjam-2.8.4 fairgbm-0.9.14 fairlearn-0.10.0 fastparquet-2024.5.0 hydra-core-1.3.2 hyperparameter-tuning-0.3.2 matplotlib-3.9.2 millify-0.1.1 narwhals-1.9.2 omegaconf-2.3.0 optuna-4.0.0 schema-0.7.7 seaborn-0.13.2 validators-0.34.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "fabc8590db20402aadb6d72aea4e37e2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dask[dataframe]"
      ],
      "metadata": {
        "id": "z9QG_fJf-8ci",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "ede87f16-587b-46e5-d32c-e6486697b9b0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dask[dataframe] in /usr/local/lib/python3.10/dist-packages (2024.8.0)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (2.2.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (24.1)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (8.4.0)\n",
            "Requirement already satisfied: pandas>=2.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (2.2.2)\n",
            "Collecting dask-expr<1.2,>=1.1 (from dask[dataframe])\n",
            "  Downloading dask_expr-1.1.15-py3-none-any.whl.metadata (2.5 kB)\n",
            "INFO: pip is looking at multiple versions of dask-expr to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading dask_expr-1.1.14-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Downloading dask_expr-1.1.13-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Downloading dask_expr-1.1.12-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Downloading dask_expr-1.1.11-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Downloading dask_expr-1.1.10-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: pyarrow>=7.0.0 in /usr/local/lib/python3.10/dist-packages (from dask-expr<1.2,>=1.1->dask[dataframe]) (16.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask[dataframe]) (3.20.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0->dask[dataframe]) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0->dask[dataframe]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0->dask[dataframe]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0->dask[dataframe]) (2024.2)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.4.0->dask[dataframe]) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0->dask[dataframe]) (1.16.0)\n",
            "Downloading dask_expr-1.1.10-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.2/242.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dask-expr\n",
            "Successfully installed dask-expr-1.1.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyarrow"
      ],
      "metadata": {
        "id": "8GIOl_lbIlkk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f09b6183-6efd-409b-bb81-eacd8b3199de"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (16.1.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QD4kgTxd-_y8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "304a02a2-d4a0-4b60-90e5-a42c9c14f74a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-MFm9TyVNpH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import lightgbm as lgbm\n",
        "import yaml              # Read hyperparameter space configuration\n",
        "import glob\n",
        "import os\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, roc_curve\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder # Categorical encoding for LGBM models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import preprocessing\n",
        "from sklearn import metrics\n",
        "\n",
        "from aequitas.group import Group                # Fairness metrics\n",
        "from random_search import RandomValueTrial, suggest_callable_hyperparams  # Random search wrapper methods"
      ]
    },
    {
      "source": [
        "import yaml\n",
        "\n",
        "# If the file is within a subdirectory, for example, named \"config\":\n",
        "with open(\"/content/lightgbm_hyperparameter_space.yaml\", \"r\") as file:\n",
        "    # Remove the 'Loader' argument\n",
        "    hyperparam_space = yaml.safe_load(file)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "sLORh8rwKZWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Bank Account Fraud (BAF) Tabular Dataset Suite"
      ],
      "metadata": {
        "id": "C9t9JHqe4_eO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "use this code to create parquet files"
      ],
      "metadata": {
        "id": "PHoC6CYDOIDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''base_path = '/content/drive/MyDrive/Mirthe Supervision Map /Paper 3a: FC Bias Disco/BAF'\n",
        "\n",
        "# Dictionary mapping dataset names to CSV paths\n",
        "datasets_paths = {\n",
        "    \"Base\": base_path + \"/Base.csv\",\n",
        "    \"Variant I\": base_path + \"/Variant I.csv\",\n",
        "    \"Variant II\": base_path + \"/Variant II.csv\",\n",
        "    \"Variant III\": base_path + \"/Variant III.csv\",\n",
        "    \"Variant IV\": base_path + \"/Variant IV.csv\",\n",
        "    \"Variant V\": base_path + \"/Variant V.csv\",\n",
        "}\n",
        "\n",
        "# Convert CSV files to Parquet\n",
        "for variant, csv_path in datasets_paths.items():\n",
        "    # Read the CSV file into a DataFrame\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Create the corresponding Parquet path by replacing .csv with .parquet\n",
        "    parquet_path = csv_path.replace(\".csv\", \".parquet\")\n",
        "\n",
        "    # Save the DataFrame as a Parquet file\n",
        "    df.to_parquet(parquet_path, engine='pyarrow')\n",
        "\n",
        "    print(f\"Converted {variant} to Parquet at {parquet_path}\")'''\n"
      ],
      "metadata": {
        "id": "22-xHxUsLFju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = '/content/drive/MyDrive/Mirthe Supervision Map /Paper 3a: FC Bias Disco/BAF'\n",
        "\n",
        "extension = \"parquet\" #for smaller \"parquet\" depending on the downloaded file\n",
        "data_paths = glob.glob(f\"{base_path}/*.{extension}\")\n",
        "\n",
        "def read_dataset(path, ext = extension):\n",
        "    if ext == \"csv\":\n",
        "      return pd.read_csv(path)\n",
        "    elif ext == \"parquet\":\n",
        "      return pd.read_parquet(path)\n",
        "    else:\n",
        "      raise ValueError(f\"Unsupported file extension: {ext}\")\n",
        "\n",
        "# Extract variant name from the file path (without the extension)\n",
        "def get_variant(path):\n",
        "    return os.path.basename(path).split(\".\")[0]\n",
        "\n",
        "# Dictionary comprehension to read all CSV files into a dictionary of DataFrames\n",
        "dataframes = {\n",
        "    get_variant(path): read_dataset(path) for path in data_paths\n",
        "}\n",
        "print(f\"Loaded datasets: {list(dataframes.keys())}\")\n",
        "\n",
        "datasets_paths = {\n",
        "    \"Base\": base_path + \"/Base.parquet\", # sampled to best represent original dataset\n",
        "    \"Variant I\": base_path + \"/Variant I.parquet\", # higher group size disparity than base - reducing the size of the minority group from approx 20 - 10% of the dataset\n",
        "    \"Variant II\": base_path + \"/Variant II.parquet\", # higher prevalance disparity than base - one group has 5 x the fraud detection rate of the other while group sizes are equal\n",
        "    \"Variant III\": base_path + \"/Variant III.parquet\", # better separability for one of the groups -\n",
        "    \"Variant IV\": base_path + \"/Variant IV.parquet\", # higher prevalance disparity in train\n",
        "    \"Variant V\": base_path + \"/Variant V.parquet\", # better separability in train for one of the groups\n",
        "}"
      ],
      "metadata": {
        "id": "-wBiCCQTfpHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = {key: read_dataset(path) for key, path in datasets_paths.items()}"
      ],
      "metadata": {
        "id": "Pi4Bg3ElBYmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataframes[\"Variant I\"].info()"
      ],
      "metadata": {
        "id": "o78df7oDh7Zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find best model"
      ],
      "metadata": {
        "id": "vLgDT4d0Plet"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5 LightGBM models with the best results (evaluation metrics accuracy or AUC) were chosen. These were trained and evaluated on the original data and involved testing 200 different sets hps (e.g., learning rate, number of leaves, max depth) that control how the model learns.\n",
        "The HP configurations were chosen through random search."
      ],
      "metadata": {
        "id": "tGp_NspMr1GC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The expected structure is the following:\n",
        "hyperparam_space"
      ],
      "metadata": {
        "id": "ar6xbnATr_Qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing a random search suggestion:\n",
        "trial = RandomValueTrial(seed=1)\n",
        "suggest_callable_hyperparams(trial, hyperparam_space)"
      ],
      "metadata": {
        "id": "-3UFCyWtPN3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the label field and categorical columns.\n",
        "label = \"fraud_bool\"\n",
        "\n",
        "categorical_features = [\n",
        "    \"payment_type\",\n",
        "    \"employment_status\",\n",
        "    \"housing_status\",\n",
        "    \"source\",\n",
        "    \"device_os\",\n",
        "]"
      ],
      "metadata": {
        "id": "diJpiOPmPhAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the train and test sets. Shuffle data with `sample` method.\n",
        "# The split was done by month. The first 6 months as the train, the last 2 months as test.\n",
        "train_dfs = {key: df[df[\"month\"]<6].sample(frac=1, replace=False) for key, df in datasets.items()}\n",
        "test_dfs = {key: df[df[\"month\"]>=6].sample(frac=1, replace=False) for key, df in datasets.items()}"
      ],
      "metadata": {
        "id": "yleUYLjxP8-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pre-process step"
      ],
      "metadata": {
        "id": "Dj2t3kRWQOIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the categorical variables in the datasets to integers.\n",
        "# This is expected by LGBM (or columns with the `categorical` data type).\n",
        "\n",
        "for name in datasets.keys():  # For each dataset in the suite\n",
        "    train = train_dfs[name]\n",
        "    test = test_dfs[name]\n",
        "\n",
        "    for feat in categorical_features:\n",
        "        encoder = LabelEncoder()\n",
        "        encoder.fit(train[feat])  # Fit an encoder to the train set.\n",
        "        train[feat] = encoder.transform(train[feat])  # Transform train set.\n",
        "        test[feat] = encoder.transform(test[feat])    # Transform test set."
      ],
      "metadata": {
        "id": "KJDjaElgQMwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell with train loop for hyperparameter tuning of an LGBM classifier across variant datasets\n",
        "\n",
        "# Define number of trials in Random search\n",
        "n_trials=100\n",
        "# Random state for sampling seeds\n",
        "np.random.seed(42)\n",
        "# Seeds for the random search sampling algorithm\n",
        "seeds = np.random.choice(list(range(1_000_000)), size=n_trials, replace=False)\n",
        "\n",
        "# Variable to store the results for each run\n",
        "runs = {}\n",
        "\n",
        "for trial in range(n_trials):\n",
        "    seed = seeds[trial]\n",
        "    trial = RandomValueTrial(seed=seed)\n",
        "    # Hyperparameters for the random search trial\n",
        "    test_hyperparams = suggest_callable_hyperparams(trial, hyperparam_space)\n",
        "    #del test_hyperparams[\"classpath\"] # Remove unnecessary key in hyperparaemters\n",
        "\n",
        "    # Update list of tested hyperparameters.\n",
        "    prev_hyperparams = runs.get(\"hyperparams\", [])\n",
        "    prev_hyperparams.append(test_hyperparams)\n",
        "    runs[\"hyperparams\"] = prev_hyperparams\n",
        "\n",
        "    for dataset_name in datasets.keys():  # Run hyperparameters on all variants of datastes\n",
        "        model = lgbm.LGBMClassifier(n_jobs=10, **test_hyperparams)  # Instantiate LGBM Model\n",
        "        X_train = train_dfs[dataset_name].drop(columns=[\"fraud_bool\"])\n",
        "        y_train = train_dfs[dataset_name][\"fraud_bool\"]\n",
        "        X_test = test_dfs[dataset_name].drop(columns=[\"fraud_bool\"])\n",
        "        y_test = test_dfs[dataset_name][\"fraud_bool\"]\n",
        "        # Fit model to training data.\n",
        "        model.fit(X_train, y_train, categorical_feature=categorical_features)\n",
        "        # Obtain predictions in test data.\n",
        "        predictions = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        # Obtain ROC curve for the predictions.\n",
        "        fprs, tprs, thresholds = metrics.roc_curve(y_test, predictions)\n",
        "        # Obtain threshold and recall. We select 5% FPR as threshold\n",
        "        threshold = np.min(thresholds[fprs==max(fprs[fprs < 0.05])])\n",
        "        recall = np.max(tprs[fprs==max(fprs[fprs < 0.05])])\n",
        "\n",
        "        # Binarize predictions for Aequitas.\n",
        "        preds_binary = (predictions > threshold).astype(int)\n",
        "\n",
        "        # Create a dataframe with protected group column, predictions and labels.\n",
        "        # Here, we select age>=50 as threshold.\n",
        "        aequitas_df = pd.DataFrame(\n",
        "            {\n",
        "                \"age\": (X_test[\"customer_age\"]>=50).map({True: \"Older\", False: \"Younger\"}),\n",
        "                \"preds\": preds_binary,\n",
        "                \"y\": y_test.values\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Obtain FPR results for different groups.\n",
        "        g = Group()\n",
        "        aequitas_results = g.get_crosstabs(aequitas_df, attr_cols=[\"age\"], score_col=\"preds\", label_col=\"y\")[0]\n",
        "\n",
        "        # Store the results for the trained model\n",
        "        results = {}\n",
        "        results[\"recall\"] = recall\n",
        "        results[\"recall Older\"] = aequitas_results[aequitas_results[\"attribute_value\"] == \"Older\"][[\"tpr\"]].values[0][0]\n",
        "        results[\"recall Younger\"] = aequitas_results[aequitas_results[\"attribute_value\"] == \"Younger\"][[\"tpr\"]].values[0][0]\n",
        "        results[\"fpr Older\"] = aequitas_results[aequitas_results[\"attribute_value\"] == \"Older\"][[\"fpr\"]].values[0][0]\n",
        "        results[\"fpr Younger\"] = aequitas_results[aequitas_results[\"attribute_value\"] == \"Younger\"][[\"fpr\"]].values[0][0]\n",
        "\n",
        "        # Store the results in the runs variable\n",
        "        prev_runs = runs.get(dataset_name, [])\n",
        "        prev_runs.append(results)\n",
        "        runs[dataset_name] = prev_runs"
      ],
      "metadata": {
        "id": "ObZYaBMnQjsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe with the results for each model in each dataset.\n",
        "rs_results = pd.DataFrame(runs)"
      ],
      "metadata": {
        "id": "uinYz8CrcNSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper method to obtain the metric values for a given model.\n",
        "def get_results(results, variant, metric):\n",
        "    col = results[variant]\n",
        "    values = []\n",
        "    for idx, val in col.iteritems():\n",
        "        values.append(val[metric])\n",
        "    return values"
      ],
      "metadata": {
        "id": "aslvOCMWcORP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain the relevant metrics to plots from the dataframe.\n",
        "variants = list(datasets_paths.keys())\n",
        "\n",
        "plot_results = {\"Variant\": [], \"Recall\": [], \"FPR Ratio\": []}\n",
        "\n",
        "for variant in variants:\n",
        "    plot_results[\"Recall\"] += get_results(rs_results, variant, \"recall\")\n",
        "    # Obtain the FPR if both groups.\n",
        "    for fpr_younger, fpr_older in zip(get_results(rs_results, variant, \"fpr Younger\"), get_results(rs_results, variant, \"fpr Older\")):\n",
        "        # Calculate FPR ratio as higher fpr / lower fpr\n",
        "        if fpr_younger > fpr_older:\n",
        "            plot_results[\"FPR Ratio\"] += [fpr_older/fpr_younger]\n",
        "        else:\n",
        "            plot_results[\"FPR Ratio\"] += [fpr_younger/fpr_older]\n",
        "    plot_results[\"Variant\"] += [variant] * len(get_results(rs_results, variant, \"recall\"))\n",
        "\n",
        "# Create a dataframe for easier plots.\n",
        "plot_results = pd.DataFrame(plot_results)"
      ],
      "metadata": {
        "id": "nTbK9Uj7cQ2Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}