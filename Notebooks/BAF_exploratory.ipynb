{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1F9XxSN05aDEQQ4Rm2cMI80wUJWDqhTfP",
      "authorship_tag": "ABX9TyMPq3vPWJT9o06DQMUeWUmF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MDankloff/Cluster-Bias-Disco/blob/main/Notebooks/BAF_exploratory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Libraries"
      ],
      "metadata": {
        "id": "t3gmpDbsrhEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/MDankloff/Cluster-Bias-Disco.git\n",
        "#!cd Cluster-Bias-Disco/\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "! cd '/content/drive/MyDrive/Mirthe_Supervision /Paper#3/BAF'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvLZyz_LKAEF",
        "outputId": "1d2d0355-c391-4d46-8ae4-a187f4fbcf7f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dask[dataframe]"
      ],
      "metadata": {
        "id": "dytoES6UqSOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import lightgbm as lgbm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import yaml\n",
        "import glob\n",
        "import os\n",
        "import joblib\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, roc_auc_score"
      ],
      "metadata": {
        "id": "FGODOIzkMzL0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data and Best LightGBM Models"
      ],
      "metadata": {
        "id": "TVC8T9ABrl_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = '/content/drive/MyDrive/Mirthe_Supervision /Paper#3/BAF/parquet data'\n",
        "\n",
        "extension = \"parquet\" #for smaller \"parquet\" depending on the downloaded file\n",
        "data_paths = glob.glob(f\"{base_path}/*.{extension}\")\n",
        "\n",
        "def read_dataset(path, ext = extension):\n",
        "    if ext == \"csv\":\n",
        "      return pd.read_csv(path)\n",
        "    elif ext == \"parquet\":\n",
        "      return pd.read_parquet(path)\n",
        "    else:\n",
        "      raise ValueError(f\"Unsupported file extension: {ext}\")\n",
        "\n",
        "# Extract variant name from the file path (without the extension)\n",
        "def get_variant(path):\n",
        "    return os.path.basename(path).split(\".\")[0]\n",
        "\n",
        "# Dictionary comprehension to read all CSV files into a dictionary of DataFrames\n",
        "dataframes = {\n",
        "    get_variant(path): read_dataset(path) for path in data_paths\n",
        "}\n",
        "print(f\"Loaded datasets: {list(dataframes.keys())}\")\n",
        "\n",
        "datasets_paths = {\n",
        "    \"Base\": base_path + \"/Base.parquet\", # sampled to best represent original dataset\n",
        "    \"Variant I\": base_path + \"/Variant I.parquet\", # higher group size disparity than base - reducing the size of the minority group from approx 20 - 10% of the dataset\n",
        "    \"Variant II\": base_path + \"/Variant II.parquet\", # higher prevalence disparity than base - one group has 5 x the fraud detection rate of the other while group sizes are equal\n",
        "    \"Variant III\": base_path + \"/Variant III.parquet\", # better separability for one of the groups -\n",
        "    \"Variant IV\": base_path + \"/Variant IV.parquet\", # higher prevalence disparity in train\n",
        "    \"Variant V\": base_path + \"/Variant V.parquet\", # better separability in train for one of the groups\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wBiCCQTfpHP",
        "outputId": "81852da6-2d02-4b83-9cfc-ec535ed70e42"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded datasets: ['Base', 'Variant I', 'Variant II', 'Variant III', 'Variant IV', 'Variant V']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Rename datasets\n",
        "base = dataframes['Base']\n",
        "variant1 = dataframes['Variant I']\n",
        "variant2 = dataframes['Variant II']\n",
        "variant3 = dataframes['Variant III']\n",
        "variant4 = dataframes['Variant IV']\n",
        "variant5 = dataframes['Variant V']"
      ],
      "metadata": {
        "id": "o78df7oDh7Zb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# directory containing the model files\n",
        "model_dir = '/content/drive/MyDrive/Mirthe_Supervision /Paper#3/BAF/Best Model per Variant'\n",
        "\n",
        "# Get list of all model files in the directory\n",
        "model_files = glob.glob(os.path.join(model_dir, '*.pkl'))\n",
        "\n",
        "# Dictionary to store loaded models\n",
        "models = {}\n",
        "\n",
        "# Load all models from the directory and save them to the dictionary\n",
        "for model_file in model_files:\n",
        "    # Load the model\n",
        "    with open(model_file, 'rb') as f:\n",
        "        model = joblib.load(f)\n",
        "\n",
        "    # Extract the model name from the file path (without extension)\n",
        "    model_name = os.path.basename(model_file).split('.')[0]\n",
        "\n",
        "    # Add the model to the dictionary\n",
        "    models[model_name] = model\n",
        "\n",
        "    # Optional: Save the model back (though it seems redundant here)\n",
        "    save_path = os.path.join(model_dir, f'{model_name}.pkl')\n",
        "    joblib.dump(model, save_path)\n",
        "\n",
        "    print(f\"Model '{model_name}' loaded and saved to: {save_path}\")\n",
        "\n",
        "# Accessing the best model per variant\n",
        "modelb = models.get(\"model_Base_top_4\")\n",
        "modelv1 = models.get(\"model_Variant I_top_4\")\n",
        "modelv2 = models.get(\"model_Variant II_top_4\")\n",
        "modelv3 = models.get(\"model_Variant III_top_0\")\n",
        "modelv4 = models.get(\"model_Variant IV_top_6\")\n",
        "modelv5 = models.get(\"model_Variant V_top_0\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3xBfZhIITLa",
        "outputId": "b9d94ca1-961b-4ad4-f276-91acfba25ae2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 'model_Base_top_4' loaded and saved to: /content/drive/MyDrive/Mirthe_Supervision /Paper#3/BAF/Best Model per Variant/model_Base_top_4.pkl\n",
            "Model 'model_Variant II_top_4' loaded and saved to: /content/drive/MyDrive/Mirthe_Supervision /Paper#3/BAF/Best Model per Variant/model_Variant II_top_4.pkl\n",
            "Model 'model_Variant III_top_0' loaded and saved to: /content/drive/MyDrive/Mirthe_Supervision /Paper#3/BAF/Best Model per Variant/model_Variant III_top_0.pkl\n",
            "Model 'model_Variant IV_top_6' loaded and saved to: /content/drive/MyDrive/Mirthe_Supervision /Paper#3/BAF/Best Model per Variant/model_Variant IV_top_6.pkl\n",
            "Model 'model_Variant V_top_0' loaded and saved to: /content/drive/MyDrive/Mirthe_Supervision /Paper#3/BAF/Best Model per Variant/model_Variant V_top_0.pkl\n",
            "Model 'model_Variant I_top_4' loaded and saved to: /content/drive/MyDrive/Mirthe_Supervision /Paper#3/BAF/Best Model per Variant/model_Variant I_top_4.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get the Errors"
      ],
      "metadata": {
        "id": "d_L6ywour2jW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dfs = {key: df[df[\"month\"] < 6].sample(frac=1, replace=False) for key, df in dataframes.items()}\n",
        "test_dfs = {key: df[df[\"month\"] >= 6].sample(frac=1, replace=False) for key, df in dataframes.items()}"
      ],
      "metadata": {
        "id": "yqXlP2hCxQav"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_errors (model, data, test_dfs):\n",
        "    \"\"\"\n",
        "    Evaluate trained model on test data and compute metrics and errors.\n",
        "    - model: Trained model (Best LightGBM model)\n",
        "    - X_test: Feature matrix for the test data\n",
        "    - Y_test: True labels for the test data\n",
        "\n",
        "    Returns:\n",
        "    - metrics: Dictionary containing calculated metrics (accuracy, precision, recall, f1, ROC AUC)\n",
        "    - predictions: DataFrame containing the true and predicted classes along with errors\n",
        "    \"\"\"\n",
        "    #get test data\n",
        "    X_test = test_dfs[data].drop(columns=[\"fraud_bool\"])\n",
        "    Y_test = test_dfs[data][\"fraud_bool\"]\n",
        "\n",
        "    #predict\n",
        "    Y_pred = model.predict(X_test)\n",
        "    Y_prob = model.predict_proba(X_test)[:, 1]  # Get probabilities for ROC AUC (for class 1)\n",
        "\n",
        "   #get evaluation metrics\n",
        "    accuracy = accuracy_score(Y_test, Y_pred)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(Y_test, Y_pred, average='binary')\n",
        "    conf_matrix = confusion_matrix(Y_test, Y_pred)\n",
        "    roc_auc = roc_auc_score(Y_test, Y_prob)\n",
        "\n",
        "    #Print the evaluation metrics & confusion matrix\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(conf_matrix)\n",
        "\n",
        "    # Plot Confusion Matrix\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Non-Fraud\", \"Fraud\"], yticklabels=[\"Non-Fraud\", \"Fraud\"])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    #get errors\n",
        "    predictions = pd.DataFrame()\n",
        "    predictions['predicted_class'] = Y_pred.tolist()  # Convert np.array to list if needed\n",
        "    predictions = predictions.set_index(X_test.index)  # Set index to match X_test\n",
        "    predictions['true_class'] = Y_test  # Add true classes\n",
        "\n",
        "    #Calculate the errors by taking the absolute difference\n",
        "    predictions['errors'] = abs(predictions['predicted_class'] - predictions['true_class'])\n",
        "\n",
        "    return {\n",
        "    'accuracy': accuracy,\n",
        "    'precision': precision,\n",
        "    'recall': recall,\n",
        "    'f1': f1,\n",
        "    'roc_auc': roc_auc\n",
        "    }, predictions"
      ],
      "metadata": {
        "id": "Q3TdatkTrfYG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_errors(modelb, \"Base\", test_dfs)"
      ],
      "metadata": {
        "id": "MSsL7scq0xJT",
        "outputId": "28a4a3f2-f286-420e-afa2-5939dfee4be2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "pandas dtypes must be int, float or bool.\nFields with bad pandas dtypes: payment_type: object, employment_status: object, housing_status: object, source: object, device_os: object",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-31a271199b27>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Base\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-431456623dd2>\u001b[0m in \u001b[0;36mget_errors\u001b[0;34m(model, data, test_dfs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mY_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Get probabilities for ROC AUC (for class 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, validate_features, **kwargs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     ):\n\u001b[1;32m   1320\u001b[0m         \u001b[0;34m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         result = self.predict_proba(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m             \u001b[0mraw_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraw_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, validate_features, **kwargs)\u001b[0m\n\u001b[1;32m   1349\u001b[0m     ):\n\u001b[1;32m   1350\u001b[0m         \u001b[0;34m\"\"\"Docstring is set after definition, using a template.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1351\u001b[0;31m         result = super().predict(\n\u001b[0m\u001b[1;32m   1352\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m             \u001b[0mraw_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraw_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, validate_features, **kwargs)\u001b[0m\n\u001b[1;32m   1034\u001b[0m         \u001b[0mpredict_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_threads\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_n_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_threads\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1036\u001b[0;31m         return self._Booster.predict(  # type: ignore[union-attr]\n\u001b[0m\u001b[1;32m   1037\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mraw_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraw_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, validate_features, **kwargs)\u001b[0m\n\u001b[1;32m   4746\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4747\u001b[0m                 \u001b[0mnum_iteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4748\u001b[0;31m         return predictor.predict(\n\u001b[0m\u001b[1;32m   4749\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4750\u001b[0m             \u001b[0mstart_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_iteration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, validate_features)\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd_DataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m             data = _data_from_pandas(\n\u001b[0m\u001b[1;32m   1140\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m_data_from_pandas\u001b[0;34m(data, feature_name, categorical_feature, pandas_categorical)\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m     return (\n\u001b[0;32m--> 848\u001b[0;31m         \u001b[0m_pandas_to_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m         \u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m         \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m_pandas_to_numpy\u001b[0;34m(data, target_dtype)\u001b[0m\n\u001b[1;32m    792\u001b[0m     \u001b[0mtarget_dtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"np.typing.DTypeLike\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m ) -> np.ndarray:\n\u001b[0;32m--> 794\u001b[0;31m     \u001b[0m_check_for_bad_pandas_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;31m# most common case (no nullable dtypes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m_check_for_bad_pandas_dtypes\u001b[0;34m(pandas_dtypes_series)\u001b[0m\n\u001b[1;32m    782\u001b[0m     ]\n\u001b[1;32m    783\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbad_pandas_dtypes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    785\u001b[0m             \u001b[0;34m'pandas dtypes must be int, float or bool.\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;34mf'Fields with bad pandas dtypes: {\", \".join(bad_pandas_dtypes)}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: pandas dtypes must be int, float or bool.\nFields with bad pandas dtypes: payment_type: object, employment_status: object, housing_status: object, source: object, device_os: object"
          ]
        }
      ]
    }
  ]
}